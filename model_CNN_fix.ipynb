{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxlVkqB1S7xB",
        "outputId": "5ff86602-396e-41db-86d0-cb9f72982856"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1jfK9HWS073"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, glob\n",
        "import librosa\n",
        "import random\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "# import librosa.display\n",
        "# import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, LSTM, Flatten, Input, Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S1DKsIlS075"
      },
      "source": [
        "Make Directory to File Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jrIIhpLuS076"
      },
      "outputs": [],
      "source": [
        "def load_dir(directory):\n",
        "    audio_dir = []\n",
        "    labels = []\n",
        "\n",
        "    for folder in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            label_file = os.path.join(folder_path, 'new_label.txt')\n",
        "            with open(label_file, 'r') as f:\n",
        "                sds_score = float(f.read().strip())\n",
        "                if sds_score < 40:\n",
        "                    label = 'low'\n",
        "                elif 41 >= sds_score <= 60:\n",
        "                    label = 'medium'\n",
        "                else:\n",
        "                    label = 'high'\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith('_out.wav'):\n",
        "                    audio_path = os.path.join(folder_path, file)\n",
        "                    audio_dir.append(audio_path)\n",
        "                    labels.append(label)\n",
        "\n",
        "    return audio_dir, labels\n",
        "\n",
        "dir = '/content/drive/MyDrive/Capstone/data/EATD-Corpus/'\n",
        "ds = load_dir(dir)\n",
        "audio_dir, labels = ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn-rxASeS077"
      },
      "source": [
        "Split directory for train, validation, test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TnSzabelS077"
      },
      "outputs": [],
      "source": [
        "# Split train dan test set (X:data, y:label)\n",
        "X_train, X_val, y_train, y_val = train_test_split(audio_dir, labels, test_size=0.2, random_state=123)\n",
        "# Split kedua untuk validation set\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6d9eMHISRng",
        "outputId": "b621f49e-5458-45ba-c5ef-ecd5ebd46bc1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zSYzc6orS077",
        "outputId": "048551e0-0e9c-455f-f647-8bc3162c6bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "388\n",
            "98\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUpLiGT5S078"
      },
      "source": [
        "PRE-PROCESS DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nay_i_LQS078"
      },
      "source": [
        "1. Resample\n",
        "2. Mono to stereo\n",
        "3. Padding\n",
        "4. Melspectrogram\n",
        "5. Melspectrogram augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mQjNxfssS078"
      },
      "outputs": [],
      "source": [
        "def open(file):\n",
        "    y, sr = librosa.load(file)\n",
        "    return (y, sr)\n",
        "\n",
        "def rechannel(audio, channel = 2):\n",
        "    y, sr = audio\n",
        "    if (y.shape[0] == channel):\n",
        "        return y\n",
        "    if (channel == 1):\n",
        "        y_rechan = y[:1, :]\n",
        "    else:\n",
        "        y_rechan = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "        y_rechan = tf.stack([y_rechan, y_rechan], axis=-1)\n",
        "        y_rechan = y_rechan.numpy()\n",
        "    return ((y_rechan, sr))\n",
        "\n",
        "def resample2(audio, nsr):\n",
        "    y, sr = audio\n",
        "    resamp = np.zeros((2, int(np.ceil(len(y) / sr * nsr))))\n",
        "    for i in range(y.shape[1]):\n",
        "        resamp_chan = librosa.resample(y[:, i], orig_sr = sr, target_sr=nsr)\n",
        "        if len(resamp_chan) > resamp.shape[1]:\n",
        "          resamp_chan = resamp_chan[:-1]\n",
        "        elif len(resamp_chan) < resamp.shape[1]:\n",
        "          resamp_chan = np.pad(resamp_chan, (0,1), mode='constant')\n",
        "        resamp[i, :] = resamp_chan\n",
        "    return((resamp, nsr))\n",
        "\n",
        "def padding_trunc(audio, max_size):\n",
        "    y, sr = audio\n",
        "    rows, y_len = y.shape\n",
        "    max_len = sr//1000 * max_size\n",
        "\n",
        "    if (y_len > max_len):\n",
        "        # lakukan truncating/potong size\n",
        "        y = y[:, :max_len]\n",
        "\n",
        "    elif (y_len < max_len):\n",
        "        #lakukan padding/penambahan size\n",
        "        len_begin = random.randint(0, max_len - y_len)\n",
        "        len_end = max_len - y_len - len_begin\n",
        "        # padding dengan zero\n",
        "        pad_begin = tf.zeros((rows, len_begin))\n",
        "        pad_end = tf.zeros((rows, len_end))\n",
        "\n",
        "        y = tf.concat([pad_begin, y, pad_end], 1)\n",
        "\n",
        "    return (y, sr)\n",
        "\n",
        "def time_shift_augment(audio, shift_limit=0.2):\n",
        "    y, sr = audio\n",
        "    _, y_len = y.shape\n",
        "\n",
        "    shift_amount = tf.random.uniform([], -shift_limit, shift_limit) * tf.cast(y_len, tf.float32)\n",
        "    shift_amount = tf.cast(shift_amount, tf.int32)\n",
        "\n",
        "    # Melakukan time shift\n",
        "    shifted_y = tf.roll(y, shift=shift_amount, axis=0)\n",
        "\n",
        "    return (shifted_y, sr)\n",
        "\n",
        "def mel_features(audio, n_mels=64, n_fft= 1024, hop_length=None):\n",
        "    y, sr = audio\n",
        "    top_db = 80\n",
        "    if hop_length is None:\n",
        "        n_fft // 2\n",
        "\n",
        "    if isinstance(y, tf.Tensor):\n",
        "        y = y.numpy()\n",
        "\n",
        "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
        "    mels_db = librosa.power_to_db(mels, ref=np.max, top_db=top_db)\n",
        "    # mels_db = tf.convert_to_tensor(mels_db, dtype=tf.float32)\n",
        "\n",
        "    return mels_db\n",
        "\n",
        "def mels_augment(mels_db, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "    _, n_mels, n_steps = mels_db.shape\n",
        "    mask_value = tf.reduce_mean(mels_db)\n",
        "    augment_mels = tf.identity(mels_db)\n",
        "\n",
        "    freq_mask_param = int(max_mask_pct * n_mels)\n",
        "    for _ in range(n_freq_masks):\n",
        "        # Membuat mask frekuensi\n",
        "        f = tf.random.uniform(shape=(), minval=0, maxval=freq_mask_param, dtype=tf.int32)\n",
        "        f0 = tf.random.uniform(shape=(), minval=0, maxval=n_mels - f, dtype=tf.int32)\n",
        "        mask = tf.concat(\n",
        "            [\n",
        "                tf.ones(shape=(f0,), dtype=tf.bool),\n",
        "                tf.zeros(shape=(f,), dtype=tf.bool),\n",
        "                tf.ones(shape=(n_mels - f0 - f,), dtype=tf.bool),\n",
        "            ],\n",
        "            0,\n",
        "        )\n",
        "        mask = tf.reshape(mask, (1, n_mels, 1))  # Ubah shape agar sesuai dengan spectrogram\n",
        "        augment_mels = tf.where(mask, mask_value, augment_mels)\n",
        "\n",
        "    time_mask_param = max(1, int(max_mask_pct * n_steps))\n",
        "    for _ in range(n_time_masks):\n",
        "        # Membuat mask waktu\n",
        "        t = tf.random.uniform(shape=(), minval=0, maxval=time_mask_param, dtype=tf.int32)\n",
        "        t0 = tf.random.uniform(shape=(), minval=0, maxval=n_steps - t, dtype=tf.int32)\n",
        "    mask = tf.concat(\n",
        "            [\n",
        "                tf.ones(shape=(t0,), dtype=tf.bool),\n",
        "                tf.zeros(shape=(t,), dtype=tf.bool),\n",
        "                tf.ones(shape=(n_steps - t0 - t,), dtype=tf.bool),\n",
        "            ],\n",
        "            0,\n",
        "        )\n",
        "\n",
        "    mask = tf.reshape(mask, (1, 1, n_steps))  # Ubah shape agar sesuai dengan spectrogram\n",
        "    augment_mels = tf.where(mask, mask_value, augment_mels)\n",
        "\n",
        "    return augment_mels\n",
        "\n",
        "def preprocess_audio(filepath):\n",
        "    nsr = 44000\n",
        "    max_size = 1000\n",
        "    audio  = open(filepath)\n",
        "    audio_stereo = rechannel(audio)\n",
        "    audio_resamp = resample2(audio_stereo, nsr)\n",
        "    audio_padd = padding_trunc(audio_resamp, max_size)\n",
        "    audio_mels = mel_features(audio_padd)\n",
        "    audio_mels_augment = mels_augment(audio_mels)\n",
        "\n",
        "    return audio_mels_augment\n",
        "\n",
        "def load_audio(dir, label):\n",
        "  features = []\n",
        "  labels = []\n",
        "  for file, label in zip(dir, label):\n",
        "    x = preprocess_audio(file)\n",
        "    features.append(x)\n",
        "    labels.append(label)\n",
        "\n",
        "  return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ii = ['/content/drive/MyDrive/Capstone/data/EATD-Corpus/t_1/negative_out.wav',\n",
        "      '/content/drive/MyDrive/Capstone/data/EATD-Corpus/t_8/positive_out.wav']\n",
        "il = [0, 0]\n",
        "\n",
        "# i = '/content/drive/MyDrive/Capstone/data/EATD-Corpus/t_8/positive_out.wav'\n",
        "# ii = ['/content/drive/MyDrive/Capstone/data/EATD-Corpus/t_8/positive_out.wav']\n",
        "# il = [0]\n",
        "\n",
        "# n5 = open(i)\n",
        "# n5 = rechannel(n5)\n",
        "# n5 = resample2(n5, nsr=44000)\n",
        "# n5 = padding_trunc(n5, max_size=1000)\n",
        "# y, sr = n5\n",
        "# y.shape\n",
        "a, b = load_audio(ii, il)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W9i0hwmI0SUe",
        "outputId": "53786eb2-28b1-4e8a-f2da-e28492ad6796"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(2, 64, 172), dtype=float64, numpy=\n",
              " array([[[-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         ...,\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124]],\n",
              " \n",
              "        [[-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         ...,\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124],\n",
              "         [-50.40157124, -50.40157124, -50.40157124, ..., -50.40157124,\n",
              "          -50.40157124, -50.40157124]]])>,\n",
              " <tf.Tensor: shape=(2, 64, 172), dtype=float64, numpy=\n",
              " array([[[-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         ...,\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008]],\n",
              " \n",
              "        [[-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         ...,\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008],\n",
              "         [-53.25944008, -53.25944008, -53.25944008, ..., -53.25944008,\n",
              "          -53.25944008, -53.25944008]]])>]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "encod = LabelEncoder()\n",
        "y_train = encod.fit_transform(y_train)\n",
        "y_val = encod.fit_transform(y_val)"
      ],
      "metadata": {
        "id": "_hBzoR27zUK1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ouQxpAUfn-",
        "outputId": "b5948f43-a7bf-43a9-84af-b6b79de36acb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [preprocess_audio(file) for file in X_train]\n",
        "X_val = [preprocess_audio(file) for file in X_val]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EgCK6m3fzjQw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([tensor.numpy().astype(np.float32) for tensor in X_train])\n",
        "X_val = np.array([tensor.numpy().astype(np.float32) for tensor in X_val])"
      ],
      "metadata": {
        "id": "yWiXXsPbT3B5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train).astype(np.int32)\n",
        "y_val = np.array(y_val).astype(np.int32)"
      ],
      "metadata": {
        "id": "EJZf1N7_VZBY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ue6XKw3YS078"
      },
      "outputs": [],
      "source": [
        "# Buat dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ew4p7eqV4ws",
        "outputId": "6fc23bff-ce4d-4134-ba97-8bda513a6626"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=(TensorSpec(shape=(2, 64, 172), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qVg9QycQS07_"
      },
      "outputs": [],
      "source": [
        "buffer_size = 310\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(32)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jQEpldQ7S08A"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    # tf.keras.Input(shape=(train_padding.shape[1], train_padding.shape[2], 1)),\n",
        "    Input(shape=(2, 64, 172)),\n",
        "    Conv2D(2, 8, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.ZeroPadding2D(padding=(1,1)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.ZeroPadding2D(padding=(1,1)),\n",
        "    Conv2D(64, (2, 2), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.ZeroPadding2D(padding=(1,1)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JrkRrs-1S08A",
        "outputId": "8c80c690-1b78-440a-d72c-1232378d90af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 2, 64, 32)         49568     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 1, 32, 32)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " zero_padding2d_3 (ZeroPadd  (None, 3, 34, 32)         0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 3, 34, 64)         18496     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 1, 17, 64)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " zero_padding2d_4 (ZeroPadd  (None, 3, 19, 64)         0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 3, 19, 64)         16448     \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPooli  (None, 1, 9, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " zero_padding2d_5 (ZeroPadd  (None, 3, 11, 64)         0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2112)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               270464    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 355363 (1.36 MB)\n",
            "Trainable params: 355363 (1.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0Ql1x-OoS08A",
        "outputId": "ae7b593e-887f-4a99-8303-f9ffaecaa734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.7575 - accuracy: 0.6985 - val_loss: 0.7403 - val_accuracy: 0.6633\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.7579 - accuracy: 0.6985 - val_loss: 0.7728 - val_accuracy: 0.6633\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7708 - accuracy: 0.6985 - val_loss: 0.7492 - val_accuracy: 0.6633\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.7624 - accuracy: 0.6985 - val_loss: 0.7296 - val_accuracy: 0.6633\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7452 - accuracy: 0.6985 - val_loss: 0.7896 - val_accuracy: 0.6633\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7863 - accuracy: 0.6985 - val_loss: 0.7255 - val_accuracy: 0.6633\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7561 - accuracy: 0.6985 - val_loss: 0.7416 - val_accuracy: 0.6633\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.8008 - accuracy: 0.6985 - val_loss: 0.7223 - val_accuracy: 0.6633\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.7900 - accuracy: 0.6804 - val_loss: 0.8481 - val_accuracy: 0.6633\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7735 - accuracy: 0.6985 - val_loss: 0.7415 - val_accuracy: 0.6633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6bb03314b0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs = 10, validation_data=val_dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}